
{
    "deck_name": "AI Engineering Level 1",
    "deck_id": 987654322,
    "cards": [
        {
            "front": "Large Language Model (LLM)",
            "back": "A type of AI model trained on vast amounts of text data to understand and generate human language. Use case: LLMs are used in chatbots, content creation, and language translation.",
            "tags": [
                "natural_language_processing",
                "deep_learning"
            ]
        },
        {
            "front": "Completion",
            "back": "The process of generating text to complete a given prompt based on learned patterns. Use case: Completion is used in predictive text, code auto-completion, and AI writing assistants.",
            "tags": [
                "natural_language_processing",
                "text_generation"
            ]
        },
        {
            "front": "Transformer",
            "back": "A deep learning architecture that uses self-attention mechanisms to process input sequences in parallel. Use case: Transformers are used in natural language processing tasks like translation and summarization.",
            "tags": [
                "deep_learning",
                "architectures"
            ]
        },
        {
            "front": "Embedding",
            "back": "A representation of data in a lower-dimensional space, capturing the semantic meaning of the data. Use case: Embeddings are used in natural language processing to represent words, sentences, or documents.",
            "tags": [
                "natural_language_processing",
                "representation"
            ]
        },
        {
            "front": "Tokenization",
            "back": "The process of splitting text into smaller pieces, such as words or subwords, for processing by a model. Use case: Tokenization is a fundamental step in preparing text data for natural language processing tasks.",
            "tags": [
                "natural_language_processing",
                "preprocessing"
            ]
        },
        {
            "front": "Self-Attention",
            "back": "A mechanism that allows a model to weigh the importance of different parts of the input data. Use case: Self-attention is crucial in transformer models for understanding context.",
            "tags": [
                "deep_learning",
                "mechanisms"
            ]
        },
        {
            "front": "Fine-Tuning",
            "back": "The process of taking a pre-trained model and training it further on a specific task. Use case: Fine-tuning is used to adapt general models to specific applications.",
            "tags": [
                "model_training",
                "customization"
            ]
        },
        {
            "front": "Zero-Shot Learning",
            "back": "A model's ability to handle tasks it has not been explicitly trained on by leveraging general knowledge. Use case: Zero-shot learning is used in scenarios where labeled data is scarce.",
            "tags": [
                "learning_methods",
                "generalization"
            ]
        },
        {
            "front": "Prompt Engineering",
            "back": "The process of designing and refining prompts to elicit the best possible responses from a model. Use case: Prompt engineering is critical for optimizing the performance of language models.",
            "tags": [
                "model_interaction",
                "optimization"
            ]
        },
        {
            "front": "Generative Adversarial Network (GAN)",
            "back": "A class of machine learning frameworks where two neural networks contest with each other to generate new data. Use case: GANs are used for generating realistic images, videos, and audio.",
            "tags": [
                "generative_models",
                "adversarial_learning"
            ]
        },
        {
            "front": "Recurrent Neural Network (RNN)",
            "back": "A type of neural network designed for sequential data, where connections between nodes form a directed graph along a sequence. Use case: RNNs are used in language modeling and time-series prediction.",
            "tags": [
                "neural_networks",
                "sequential_data"
            ]
        },
        {
            "front": "BERT (Bidirectional Encoder Representations from Transformers)",
            "back": "A pre-trained transformer-based model designed to understand the context of a word in search queries. Use case: BERT is used in tasks like question answering and text classification.",
            "tags": [
                "pretrained_models",
                "contextual_models"
            ]
        },
        {
            "front": "Hyperparameter Optimization",
            "back": "The process of tuning the hyperparameters of a model to improve its performance. Use case: Hyperparameter optimization is crucial for enhancing model accuracy and efficiency.",
            "tags": [
                "model_training",
                "optimization"
            ]
        },
        {
            "front": "Attention Mechanism",
            "back": "A component of neural networks that dynamically focuses on relevant parts of the input. Use case: Attention mechanisms are used to improve the performance of models on tasks like translation and summarization.",
            "tags": [
                "mechanisms",
                "enhancements"
            ]
        },
        {
            "front": "Autoencoder",
            "back": "A type of neural network used to learn efficient codings of input data, typically for the purpose of dimensionality reduction or feature learning. Use case: Autoencoders are used in anomaly detection and image denoising.",
            "tags": [
                "representation_learning",
                "dimensionality_reduction"
            ]
        },
        {
            "front": "Beam Search",
            "back": "An algorithm that explores a graph by expanding the most promising node in a limited set. Use case: Beam search is used in sequence generation tasks such as machine translation and speech recognition.",
            "tags": [
                "search_algorithms",
                "sequence_generation"
            ]
        },
        {
            "front": "Sentence Embedding",
            "back": "A dense vector representation of a sentence, capturing its semantic meaning. Use case: Sentence embeddings are used in tasks like semantic similarity and information retrieval.",
            "tags": [
                "representation",
                "semantic_analysis"
            ]
        },
        {
            "front": "Transfer Learning",
            "back": "A technique where a pre-trained model is adapted to a new, but related task. Use case: Transfer learning is used to improve performance on tasks with limited data.",
            "tags": [
                "model_training",
                "adaptation"
            ]
        },
        {
            "front": "Data Augmentation",
            "back": "A technique to increase the diversity of training data by applying random transformations. Use case: Data augmentation is used to improve model generalization and prevent overfitting.",
            "tags": [
                "data_preprocessing",
                "model_training"
            ]
        },
        {
            "front": "Model Pruning",
            "back": "The process of removing unnecessary parameters from a neural network to reduce its size and increase inference speed. Use case: Model pruning is used to deploy efficient models on resource-constrained devices.",
            "tags": [
                "model_optimization",
                "efficiency"
            ]
        },
        {
            "front": "GPT (Generative Pre-trained Transformer)",
            "back": "A state-of-the-art language model developed by OpenAI that generates human-like text based on a given prompt. Use case: GPT is used in chatbots, content creation, and text completion.",
            "tags": [
                "language_models",
                "generative_models"
            ]
        },
        {
            "front": "Claude",
            "back": "An AI assistant developed by Anthropic focused on providing safe and interpretable outputs. Use case: Claude is used for conversational AI and providing detailed, safe responses in various applications.",
            "tags": [
                "language_models",
                "ai_assistants"
            ]
        },
        {
            "front": "T5 (Text-to-Text Transfer Transformer)",
            "back": "A model developed by Google that treats every NLP problem as a text-to-text task. Use case: T5 is used for translation, summarization, and question answering.",
            "tags": [
                "language_models",
                "text_generation"
            ]
        },
        {
            "front": "BERT (Bidirectional Encoder Representations from Transformers)",
            "back": "A pre-trained transformer-based model designed to understand the context of a word in search queries. Use case: BERT is used in tasks like question answering and text classification.",
            "tags": [
                "pretrained_models",
                "contextual_models"
            ]
        },
        {
            "front": "RoBERTa (Robustly Optimized BERT Pretraining Approach)",
            "back": "An optimized version of BERT by Facebook AI, designed to improve on the original BERT with more robust training. Use case: RoBERTa is used in NLP tasks requiring deep contextual understanding.",
            "tags": [
                "pretrained_models",
                "contextual_models"
            ]
        },
        {
            "front": "XLNet",
            "back": "A generalized autoregressive pretraining method that captures bidirectional context. Use case: XLNet is used for natural language understanding tasks like sentiment analysis and text classification.",
            "tags": [
                "language_models",
                "autoregressive_models"
            ]
        },
        {
            "front": "ERNIE (Enhanced Representation through Knowledge Integration)",
            "back": "A language model developed by Baidu that incorporates knowledge graphs for better language understanding. Use case: ERNIE is used in NLP tasks that benefit from integrating world knowledge.",
            "tags": [
                "language_models",
                "knowledge_graphs"
            ]
        },
        {
            "front": "Albert (A Lite BERT)",
            "back": "A lighter and faster version of BERT with parameter sharing to reduce model size. Use case: Albert is used for NLP tasks in environments with limited computational resources.",
            "tags": [
                "pretrained_models",
                "efficient_models"
            ]
        },
        {
            "front": "GPT-3",
            "back": "The third generation of OpenAI's Generative Pre-trained Transformer, known for its large scale and capability to perform a variety of NLP tasks. Use case: GPT-3 is used for language translation, question answering, and creative writing.",
            "tags": [
                "language_models",
                "generative_models"
            ]
        },
        {
            "front": "DistilBERT",
            "back": "A smaller, faster, cheaper version of BERT that retains 97% of BERT's language understanding while being lighter. Use case: DistilBERT is used for efficient NLP applications where model size and speed are critical.",
            "tags": [
                "pretrained_models",
                "efficient_models"
            ]
        }
    ]
}
